{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dariochemoli/deeplearning/blob/main/notebooks/3_deep_q_network_dqn_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch crash course (part II)"
      ],
      "metadata": {
        "id": "apqDw5_0ZYUK"
      },
      "id": "apqDw5_0ZYUK"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "\n",
        "th.cuda.is_available()  # Use GPU for this part if you can"
      ],
      "metadata": {
        "id": "CzMR0iKKbUcm",
        "outputId": "edceef90-9428-4126-e7c0-201c5eac43a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CzMR0iKKbUcm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.ones(3, requires_grad=True)  # A tensor I can compute gradients of\n",
        "\n",
        "print(x)\n",
        "\n",
        "def f(x):  # A differentiable function\n",
        "  return th.sum(x**2)\n",
        "\n",
        "y = f(x)  # A result I can differentiate\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ANYGKMGfZooR",
        "outputId": "454eeed9-4f3f-43ae-a681-771d2bcf69fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ANYGKMGfZooR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.], requires_grad=True)\n",
            "tensor(3., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(retain_graph=True)  # Compute gradient of y with respect to all of its inputs\n",
        "x.grad  # Retrieve the gradient of y w.r.t. x"
      ],
      "metadata": {
        "id": "SFnFuiIpaJly",
        "outputId": "45edaa3e-c9b9-494a-a7c6-e87cecc61966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SFnFuiIpaJly",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if you run the above cell multiple times?"
      ],
      "metadata": {
        "id": "g1Vy2s99czok"
      },
      "id": "g1Vy2s99czok"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad = None  # Clear previously computed gradients (otherwise they cumulate)\n",
        "y.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "PrUtusELc29N",
        "outputId": "9f35b060-0ad3-4185-9046-2f97b8e508a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PrUtusELc29N",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is another way to compute gradients"
      ],
      "metadata": {
        "id": "dWVsfNdOeqWk"
      },
      "id": "dWVsfNdOeqWk"
    },
    {
      "cell_type": "code",
      "source": [
        "z = th.ones(3, requires_grad=True)\n",
        "\n",
        "def g(x):\n",
        "  return th.sum(x**3)\n",
        "\n",
        "w = g(z)\n",
        "\n",
        "th.autograd.grad(w, z)"
      ],
      "metadata": {
        "id": "ikB-J3h6dYlN",
        "outputId": "22dd53fb-1053-48d7-e205-badb1726f2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ikB-J3h6dYlN",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3., 3., 3.]),)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is the functional (jax-style) way:"
      ],
      "metadata": {
        "id": "C8t9R1kce04k"
      },
      "id": "C8t9R1kce04k"
    },
    {
      "cell_type": "code",
      "source": [
        "dg = th.func.grad(g)\n",
        "dg(z)"
      ],
      "metadata": {
        "id": "Nzhr0_Zwe8g2",
        "outputId": "52c461aa-b5e2-4265-bedb-ce8f979c1211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Nzhr0_Zwe8g2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3., 3.], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always keep in mind that keeping track of differentiable functions for computing gradients is computationally expensive (https://docs.pytorch.org/tutorials/beginner/understanding_leaf_vs_nonleaf_tutorial.html)"
      ],
      "metadata": {
        "id": "5447Y2Q0fb51"
      },
      "id": "5447Y2Q0fb51"
    },
    {
      "cell_type": "code",
      "source": [
        "with th.no_grad():  # If you don't care about gradients w.r.t. v, you can spare pytorch some time\n",
        "  v = f(g(f(g(x))**3 / f(g(f(z)))))\n",
        "v"
      ],
      "metadata": {
        "id": "05oFj5Hzf0ix",
        "outputId": "094ad230-9ab0-4f6c-eea9-483d6c978e46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "05oFj5Hzf0ix",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical situation: compute the gradients of a model w.r.t its parameters for some input"
      ],
      "metadata": {
        "id": "llmRMPG9iAe9"
      },
      "id": "llmRMPG9iAe9"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input = th.randn(5)  # a random example\n",
        "\n",
        "target = f(input)  # a target value our model is trying to predict (regression)\n",
        "\n",
        "model = nn.Sequential(  # a small NN model\n",
        "    nn.Linear(5, 2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2, 1)\n",
        ")\n",
        "\n",
        "loss = (model(input) - target)**2  # squared loss\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "y0MilvIagzu6",
        "outputId": "ff9afd84-9c85-430b-f637-c49766c3d10f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y0MilvIagzu6",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([72.5069], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's have a look into the model"
      ],
      "metadata": {
        "id": "cUNLDuFRjxqh"
      },
      "id": "cUNLDuFRjxqh"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "\n",
        "with th.no_grad():\n",
        "\n",
        "  # Model parameters\n",
        "  params = model.parameters()\n",
        "\n",
        "  print()\n",
        "  print([p for p in params])\n",
        "\n",
        "  # Model parameters with their names\n",
        "  named_params = model.named_parameters()\n",
        "\n",
        "  print()\n",
        "  print([p for p in named_params])\n",
        "\n",
        "\n",
        "  # Model parameters as a single flat tensor\n",
        "  flat_params = parameters_to_vector(model.parameters())\n",
        "\n",
        "  print()\n",
        "  print(flat_params)"
      ],
      "metadata": {
        "id": "MnCrFpbVj1JY",
        "outputId": "d862765c-7acd-4c11-f06f-6e62729d3a84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MnCrFpbVj1JY",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Parameter containing:\n",
            "tensor([[ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106],\n",
            "        [-0.2026, -0.1030,  0.2684, -0.0336,  0.4197]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.3245, -0.2902], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.3609, -0.1840]], requires_grad=True), Parameter containing:\n",
            "tensor([0.6434], requires_grad=True)]\n",
            "\n",
            "[('0.weight', Parameter containing:\n",
            "tensor([[ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106],\n",
            "        [-0.2026, -0.1030,  0.2684, -0.0336,  0.4197]], requires_grad=True)), ('0.bias', Parameter containing:\n",
            "tensor([-0.3245, -0.2902], requires_grad=True)), ('2.weight', Parameter containing:\n",
            "tensor([[ 0.3609, -0.1840]], requires_grad=True)), ('2.bias', Parameter containing:\n",
            "tensor([0.6434], requires_grad=True))]\n",
            "\n",
            "tensor([ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106, -0.2026, -0.1030,  0.2684,\n",
            "        -0.0336,  0.4197, -0.3245, -0.2902,  0.3609, -0.1840,  0.6434])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's manually perform one step of gradient descent"
      ],
      "metadata": {
        "id": "6TkjuPYOlT03"
      },
      "id": "6TkjuPYOlT03"
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 1e-3 # Step size for gradient descent\n",
        "\n",
        "loss = (model(input) - target)**2\n",
        "\n",
        "with th.no_grad():\n",
        "  print(\"Before:\", parameters_to_vector(model.parameters()))\n",
        "\n",
        "  model.zero_grad()  # Clears the gradients of all the parameters of the model\n",
        "\n",
        "  loss.backward(retain_graph=True)\n",
        "\n",
        "  theta = parameters_to_vector(model.parameters())\n",
        "\n",
        "  grad_theta = parameters_to_vector(p.grad for p in model.parameters())\n",
        "\n",
        "  theta_new = theta - step_size * grad_theta  # Gradient descent (1 step)\n",
        "\n",
        "  vector_to_parameters(theta_new, model.parameters())  # Assign the new parameters to the model\n",
        "\n",
        "  print(\"After:\", parameters_to_vector(model.parameters()))"
      ],
      "metadata": {
        "id": "i0yuBBVPlSA1",
        "outputId": "7b0cac6d-993a-4ff7-89f5-fb7fbf74b604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i0yuBBVPlSA1",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: tensor([ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106, -0.2026, -0.1030,  0.2684,\n",
            "        -0.0336,  0.4197, -0.3245, -0.2902,  0.3609, -0.1840,  0.6434])\n",
            "After: tensor([ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106, -0.2040, -0.1024,  0.2664,\n",
            "        -0.0360,  0.4109, -0.3245, -0.2933,  0.3609, -0.1677,  0.6604])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch optimizers allow to automatize the above procedure"
      ],
      "metadata": {
        "id": "AOGG678AY0PV"
      },
      "id": "AOGG678AY0PV"
    },
    {
      "cell_type": "code",
      "source": [
        "loss = (model(input) - target)**2\n",
        "\n",
        "with th.no_grad():\n",
        "  print(\"Before:\", parameters_to_vector(model.parameters()))\n",
        "\n",
        "  optimizer = th.optim.SGD(model.parameters(), lr=step_size)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward(retain_graph=True)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"After:\", parameters_to_vector(model.parameters()))"
      ],
      "metadata": {
        "id": "uAd35ppPZS_A",
        "outputId": "e9ba1a43-1b38-4886-c2cb-cf833f8e55f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uAd35ppPZS_A",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: tensor([ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106, -0.2040, -0.1024,  0.2664,\n",
            "        -0.0360,  0.4109, -0.3245, -0.2933,  0.3609, -0.1677,  0.6604])\n",
            "After: tensor([ 0.3205,  0.3972, -0.3488, -0.0597, -0.3106, -0.2053, -0.1018,  0.2645,\n",
            "        -0.0381,  0.4030, -0.3245, -0.2961,  0.3609, -0.1520,  0.6774])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5",
      "metadata": {
        "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5"
      },
      "source": [
        "#DQN Tutorial: Deep Q-Network (DQN)\n",
        "\n",
        "## Part II: DQN Update and Training Loop\n",
        "\n",
        "Based on the RLSS 2023 tutorial by Antonin Raffin https://github.com/araffin/rlss23-dqn-tutorial\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/dqn.png\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this notebook, you will finish the implementation of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm (started in part I) by implementing the training loop and the DQN gradient update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe1a9aa-5735-4614-9a76-031656397899",
      "metadata": {
        "id": "7fe1a9aa-5735-4614-9a76-031656397899"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# !pip install jupyter-black\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8188798b-daf5-43a7-91ec-a7a922bc2034",
      "metadata": {
        "id": "8188798b-daf5-43a7-91ec-a7a922bc2034"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
      "metadata": {
        "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
        "outputId": "69852b72-8ad4-4100-8c78-6da34db40628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/araffin/rlss23-dqn-tutorial/\n",
            "  Cloning https://github.com/araffin/rlss23-dqn-tutorial/ to /tmp/pip-req-build-zi0b4267\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/araffin/rlss23-dqn-tutorial/ /tmp/pip-req-build-zi0b4267\n",
            "  Resolved https://github.com/araffin/rlss23-dqn-tutorial/ to commit ee0e4ff5dfe1c6502ebe4be744a40101e07f3f3b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.0.2)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.6.1)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.9.0+cu128)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.1.0,>=0.29.1->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.1.0,>=0.29.1->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.1.0,>=0.29.1->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.6.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (4.13.0.92)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (4.67.3)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (0.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[classic-control,other]<1.1.0,>=0.29.1->dqn_tutorial==0.1.dev60+gee0e4ff5d) (2026.1.4)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: dqn_tutorial\n",
            "  Building wheel for dqn_tutorial (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dqn_tutorial: filename=dqn_tutorial-0.1.dev60+gee0e4ff5d-py3-none-any.whl size=19921 sha256=a2b16bc0fe88ee16a348fc68ac979d5d82318f16b74dcf57a0a56e0866296f54\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tx_752td/wheels/af/2e/9c/0f1eb59648e2b461f63a0651fabe409295f36419acefd51238\n",
            "Successfully built dqn_tutorial\n",
            "Installing collected packages: gymnasium, dqn_tutorial\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.3\n",
            "    Uninstalling gymnasium-1.2.3:\n",
            "      Successfully uninstalled gymnasium-1.2.3\n",
            "Successfully installed dqn_tutorial-0.1.dev60+gee0e4ff5d gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/araffin/rlss23-dqn-tutorial/ --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "600ca128-eaab-4cda-a378-8b122bb214ca",
      "metadata": {
        "id": "600ca128-eaab-4cda-a378-8b122bb214ca",
        "outputId": "939c8814-f28a-4b12-8c5e-a1727e43c662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg  # For visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30fc32dc-45f6-4303-ab92-4d0f4cc34c67",
      "metadata": {
        "id": "30fc32dc-45f6-4303-ab92-4d0f4cc34c67"
      },
      "source": [
        "### Imports (from Part I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad",
      "metadata": {
        "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# We implemented those components in part I\n",
        "from dqn_tutorial.dqn import ReplayBuffer, epsilon_greedy_action_selection, collect_one_step, linear_schedule, QNetwork\n",
        "from dqn_tutorial.dqn.evaluation import evaluate_policy\n",
        "from dqn_tutorial.notebook_utils import show_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "24f1c3fc-078e-43b2-984f-a6bde4ce5796",
      "metadata": {
        "id": "24f1c3fc-078e-43b2-984f-a6bde4ce5796"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import dqn_tutorial\n",
        "\n",
        "video_folder = Path(dqn_tutorial.__file__).parent.parent  / \"logs/videos\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82559d16-3fc6-4d47-a373-f94adcab7102",
      "metadata": {
        "id": "82559d16-3fc6-4d47-a373-f94adcab7102"
      },
      "source": [
        "## DQN Update rule (no target network)\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/annotated_dqn.png\" width=\"1000\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41c2092-ffa8-4369-a5ac-e9b81fd2c12f",
      "metadata": {
        "id": "c41c2092-ffa8-4369-a5ac-e9b81fd2c12f"
      },
      "source": [
        "### Exercise 8 (15 minutes): write DQN update\n",
        "\n",
        "**HINT**: DQN update is heavily inspired by FQI update: if you get stuck, you can take a look at what you did in the first notebook on FQI\n",
        "\n",
        "**HINT**: The data sampled from the replay buffer uses the following structure:\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class ReplayBufferSamples:\n",
        "    \"\"\"\n",
        "    A dataclass containing transitions from the replay buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    observations: np.ndarray  # same as states in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray\n",
        "```\n",
        "\n",
        "**HINT**: You can take a look at the section about Q-Network in the second notebook (DQN part I) to recall how to predict q-values using a q-network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7",
      "metadata": {
        "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7"
      },
      "outputs": [],
      "source": [
        "from dqn_tutorial.dqn.dqn_no_target import dqn_update_no_target as cheat_dqn_update_no_target\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def dqn_update_no_target(\n",
        "    q_net: QNetwork,\n",
        "    optimizer: th.optim.Optimizer,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    batch_size: int,\n",
        "    gamma: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform one gradient step on the Q-network\n",
        "    using the data from the replay buffer.\n",
        "    Note: this is the same as dqn_update in dqn.py, but without the target network.\n",
        "\n",
        "    :param q_net: The Q-network to update\n",
        "    :param optimizer: The optimizer to use\n",
        "    :param replay_buffer: The replay buffer containing the transitions\n",
        "    :param batch_size: The minibatch size, how many transitions to sample\n",
        "    :param gamma: The discount factor\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Sample the replay buffer and convert them to PyTorch tensors\n",
        "    # using `.to_torch()` method\n",
        "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
        "\n",
        "    # We should not compute gradient with respect to the target\n",
        "    with th.no_grad():\n",
        "        # Compute the Q-values for the next observations\n",
        "        # (replay_data.next_observations)\n",
        "        # (batch_size, n_actions)\n",
        "        next_q_values = q_net(replay_data.next_observations)\n",
        "\n",
        "        # Follow greedy policy: use the one with the highest value\n",
        "        # shape: (batch_size,)\n",
        "        # Note: tensor.max(dim=..) returns a tuple (max, indices) in PyTorch\n",
        "        next_q_values_max, _ = next_q_values.max(dim=1)\n",
        "\n",
        "\n",
        "        # If the episode is terminated, set the target to the reward\n",
        "        # (same as FQI, you can use `th.logical_not` to mask the next q values)\n",
        "        not_terminated = th.logical_not(replay_data.terminateds).float()\n",
        "\n",
        "        # 1-step TD target (TD(0) same as for FQI)\n",
        "        td_target = replay_data.rewards + gamma * not_terminated * next_q_values_max\n",
        "\n",
        "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
        "    current_q_all = q_net(replay_data.observations)\n",
        "\n",
        "    # Select the Q-values corresponding to the actions that were selected\n",
        "    # during data collection,\n",
        "    # you should use `th.gather()`\n",
        "    current_q_values = th.gather(\n",
        "        current_q_all,\n",
        "        dim=1,\n",
        "        index=replay_data.actions.long())\n",
        "\n",
        "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
        "    # You can use `tensor.squeeze(dim=..)`\n",
        "    current_q_values = current_q_values.squeeze(dim=1)\n",
        "\n",
        "    # Check for any shape/broadcast error\n",
        "    # Current q-values must have the same shape as the TD target\n",
        "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
        "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) loss\n",
        "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
        "    loss = F.mse_loss(current_q_values, td_target)\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the q-network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c7b067-cd2d-4dbb-a6b5-dd7cfb515137",
      "metadata": {
        "id": "d4c7b067-cd2d-4dbb-a6b5-dd7cfb515137"
      },
      "source": [
        "Let's test the implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ab535b42-e643-422a-8087-51311b4cdb54",
      "metadata": {
        "id": "ab535b42-e643-422a-8087-51311b4cdb54"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "q_net = QNetwork(env.observation_space, env.action_space)\n",
        "optimizer = th.optim.Adam(q_net.parameters(), lr=0.001)\n",
        "replay_buffer = ReplayBuffer(2000, env.observation_space, env.action_space)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "# Let's collect some data following an epsilon-greedy policy\n",
        "for _ in range(1000):\n",
        "    obs = collect_one_step(env, q_net, replay_buffer, obs, exploration_rate=0.1)\n",
        "\n",
        "# Try to do some gradient steps:\n",
        "for _ in range(10):\n",
        "    dqn_update_no_target(q_net, optimizer, replay_buffer, batch_size=32, gamma=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515755c2-f802-4662-8337-918d1505fc9a",
      "metadata": {
        "id": "515755c2-f802-4662-8337-918d1505fc9a"
      },
      "source": [
        "### Exercise 9 (15 minutes): write the training loop\n",
        "\n",
        "Let's put everything together and implement the training loop that alternates between data collection and updating the Q-Network.\n",
        "At first we will not use any target network.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/dqn_loop.png\" width=\"600\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790401fe-f7cc-4338-a6d4-b7996d275dd2",
      "metadata": {
        "id": "790401fe-f7cc-4338-a6d4-b7996d275dd2"
      },
      "outputs": [],
      "source": [
        "from dqn_tutorial.dqn.dqn_no_target import run_dqn_no_target as cheat_run_dqn_no_target\n",
        "\n",
        "def run_dqn_no_target(\n",
        "    env_id: str = \"CartPole-v1\",\n",
        "    replay_buffer_size: int = 50_000,\n",
        "    # Exploration schedule\n",
        "    # (for the epsilon-greedy data collection)\n",
        "    exploration_initial_eps: float = 1.0,\n",
        "    exploration_final_eps: float = 0.01,\n",
        "    n_timesteps: int = 20_000,\n",
        "    update_interval: int = 2,\n",
        "    learning_rate: float = 3e-4,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    n_eval_episodes: int = 10,\n",
        "    evaluation_interval: int = 1000,\n",
        "    eval_exploration_rate: float = 0.0,\n",
        "    seed: int = 2023,\n",
        "    # device: Union[th.device, str] = \"cpu\",\n",
        "    eval_render_mode: Optional[str] = None,  # \"human\", \"rgb_array\", None\n",
        ") -> QNetwork:\n",
        "    \"\"\"\n",
        "    Run Deep Q-Learning (DQN) on a given environment.\n",
        "    (without target network)\n",
        "\n",
        "    :param env_id: Name of the environment\n",
        "    :param replay_buffer_size: Max capacity of the replay buffer\n",
        "    :param exploration_initial_eps: The initial exploration rate\n",
        "    :param exploration_final_eps: The final exploration rate\n",
        "    :param n_timesteps: Number of timesteps in total\n",
        "    :param update_interval: How often to update the Q-network\n",
        "        (every update_interval steps)\n",
        "    :param learning_rate: The learning rate to use for the optimizer\n",
        "    :param batch_size: The minibatch size\n",
        "    :param gamma: The discount factor\n",
        "    :param n_eval_episodes: The number of episodes to evaluate the policy on\n",
        "    :param evaluation_interval: How often to evaluate the policy\n",
        "    :param eval_exploration_rate: The exploration rate to use during evaluation\n",
        "    :param seed: Random seed for the pseudo random generator\n",
        "    :param eval_render_mode: The render mode to use for evaluation\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    # Seed Numpy as PyTorch pseudo random generators\n",
        "    # Seed Numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    th.manual_seed(seed)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym.make(env_id)\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    assert isinstance(env.action_space, spaces.Discrete)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(env_id, render_mode=eval_render_mode)\n",
        "    eval_env.reset(seed=seed)\n",
        "    eval_env.action_space.seed(seed)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # 1. Instantiate the Q-Network and the optimizer\n",
        "    # 2. Instantiate the replay buffer\n",
        "    # 3. Compute the current exploration rate (epsilon)\n",
        "    # 4. Collect new transition by stepping in the env following\n",
        "    # an epsilon-greedy strategy\n",
        "    # 5. Update the Q-Network using gradient descent\n",
        "\n",
        "    # Create the q-network\n",
        "\n",
        "    # Create the optimizer (PyTorch `th.optim.Adam` will be helpful here)\n",
        "\n",
        "    # Create the Replay buffer\n",
        "\n",
        "    # Reset the env\n",
        "\n",
        "    for current_step in range(1, n_timesteps + 1):\n",
        "        # Compute the current exploration rate\n",
        "        # according to the exploration schedule (update the value of epsilon)\n",
        "        # you should use `linear_schedule()`\n",
        "\n",
        "        # Do one step in the environment following an epsilon-greedy policy\n",
        "        # and store the transition in the replay buffer\n",
        "        # you can re-use `collect_one_step()`\n",
        "\n",
        "\n",
        "        # Update the Q-Network every `update_interval` steps\n",
        "        if (current_step % update_interval) == 0:\n",
        "            # Do one gradient step (using `dqn_update_no_target()`)\n",
        "            ...\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "        if (current_step % evaluation_interval) == 0:\n",
        "            print()\n",
        "            print(f\"Evaluation at step {current_step}:\")\n",
        "            # Evaluate the current greedy policy (deterministic policy)\n",
        "            evaluate_policy(eval_env, q_net, n_eval_episodes, eval_exploration_rate=eval_exploration_rate)\n",
        "    return q_net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c35cd4-9d23-4083-9384-482fdaf1e9e6",
      "metadata": {
        "id": "79c35cd4-9d23-4083-9384-482fdaf1e9e6"
      },
      "source": [
        "## Train a DQN agent on CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d24455f-5cb6-4b2c-8b48-2f395bbce8e1",
      "metadata": {
        "id": "3d24455f-5cb6-4b2c-8b48-2f395bbce8e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# create log folder\n",
        "os.makedirs(\"../logs/\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3967c2a6-469f-4766-9911-fab855f945d8",
      "metadata": {
        "id": "3967c2a6-469f-4766-9911-fab855f945d8"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "q_net = run_dqn_no_target(env_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acb6e99-8328-4c6c-be81-32845d4d3dda",
      "metadata": {
        "id": "5acb6e99-8328-4c6c-be81-32845d4d3dda"
      },
      "source": [
        "### Record and show video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b988f9-05b0-46e3-a143-d43162d5cd7a",
      "metadata": {
        "id": "e4b988f9-05b0-46e3-a143-d43162d5cd7a"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.0\n",
        "video_name = f\"DQN_no_target_{env_id}\"\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(video_folder, prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b269a237-a0db-40a4-9621-449538c410b7",
      "metadata": {
        "id": "b269a237-a0db-40a4-9621-449538c410b7"
      },
      "source": [
        "## [Bonus] DQN Target Network\n",
        "\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/target_q_network.png\" width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "The only things that is changing is when predicting the next q value.\n",
        "\n",
        "In DQN without target, the online network with weights **$\\theta$** is used:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta))$\n",
        "\n",
        "\n",
        "whereas with DQN with target network, the target q-network (a delayed copy of the q-network) with weights **$\\theta^\\prime$** is used instead:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta^\\prime))$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781e7e78-b156-43f0-aa4d-f7c842d3d837",
      "metadata": {
        "id": "781e7e78-b156-43f0-aa4d-f7c842d3d837"
      },
      "source": [
        "### Bonus exercise: write the DQN update with target network\n",
        "\n",
        "**HINT**: it is exactly the same as `dqn_update_no_target` except for computing the next q-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f17152-59cf-495a-bde7-73b6f219e779",
      "metadata": {
        "id": "27f17152-59cf-495a-bde7-73b6f219e779"
      },
      "outputs": [],
      "source": [
        "from dqn_tutorial.dqn.dqn import dqn_update as cheat_dqn_update\n",
        "\n",
        "def dqn_update(\n",
        "    q_net: QNetwork,\n",
        "    q_target_net: QNetwork,\n",
        "    optimizer: th.optim.Optimizer,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    batch_size: int,\n",
        "    gamma: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform one gradient step on the Q-network\n",
        "    using the data from the replay buffer.\n",
        "\n",
        "    :param q_net: The Q-network to update\n",
        "    :param q_target_net: The target Q-network, to compute the td-target.\n",
        "    :param optimizer: The optimizer to use\n",
        "    :param replay_buffer: The replay buffer containing the transitions\n",
        "    :param batch_size: The minibatch size, how many transitions to sample\n",
        "    :param gamma: The discount factor\n",
        "    \"\"\"\n",
        "\n",
        "    # Sample the replay buffer and convert them to PyTorch tensors\n",
        "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
        "\n",
        "    with th.no_grad():\n",
        "        ### YOUR CODE HERE\n",
        "        # TODO: use the target q-network instead of the online q-network\n",
        "        # to compute the next values\n",
        "\n",
        "        # Compute the Q-values for the next observations (batch_size, n_actions)\n",
        "        # using the target network\n",
        "\n",
        "        ...\n",
        "\n",
        "        # Follow greedy policy: use the one with the highest value\n",
        "        # (batch_size,)\n",
        "\n",
        "        # If the episode is terminated, set the target to the reward\n",
        "\n",
        "        # 1-step TD target\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
        "    q_values = q_net(replay_data.observations)\n",
        "    # Select the Q-values corresponding to the actions that were selected\n",
        "    # during data collection\n",
        "    current_q_values = th.gather(q_values, dim=1, index=replay_data.actions)\n",
        "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
        "    current_q_values = current_q_values.squeeze(dim=1)\n",
        "\n",
        "    # Check for any shape/broadcast error\n",
        "    # Current q-values must have the same shape as the TD target\n",
        "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
        "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) loss\n",
        "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
        "    loss = ((current_q_values - td_target) ** 2).mean()\n",
        "    # Huber loss\n",
        "    # loss = th.nn.functional.smooth_l1_loss(current_q_values, td_target)\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the q-network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c74b0e-1923-4c6e-9a58-f83a97435f0d",
      "metadata": {
        "id": "85c74b0e-1923-4c6e-9a58-f83a97435f0d"
      },
      "source": [
        "### Updated training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484",
      "metadata": {
        "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484"
      },
      "outputs": [],
      "source": [
        "def run_dqn(\n",
        "    env_id: str = \"CartPole-v1\",\n",
        "    replay_buffer_size: int = 50_000,\n",
        "    # How often do we copy the parameters from the Q-network to the target network\n",
        "    target_network_update_interval: int = 1000,\n",
        "    # Warmup phase\n",
        "    learning_starts: int = 100,\n",
        "    # Exploration schedule\n",
        "    # (for the epsilon-greedy data collection)\n",
        "    exploration_initial_eps: float = 1.0,\n",
        "    exploration_final_eps: float = 0.01,\n",
        "    exploration_fraction: float = 0.1,\n",
        "    n_timesteps: int = 20_000,\n",
        "    update_interval: int = 2,\n",
        "    learning_rate: float = 3e-4,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    n_hidden_units: int = 64,\n",
        "    n_eval_episodes: int = 10,\n",
        "    evaluation_interval: int = 1000,\n",
        "    eval_exploration_rate: float = 0.0,\n",
        "    seed: int = 2023,\n",
        "    # device: Union[th.device, str] = \"cpu\",\n",
        "    eval_render_mode: Optional[str] = None,  # \"human\", \"rgb_array\", None\n",
        ") -> QNetwork:\n",
        "    \"\"\"\n",
        "    Run Deep Q-Learning (DQN) on a given environment.\n",
        "    (with a target network)\n",
        "\n",
        "    :param env_id: Name of the environment\n",
        "    :param replay_buffer_size: Max capacity of the replay buffer\n",
        "    :param target_network_update_interval: How often do we copy the parameters\n",
        "         to the target network\n",
        "    :param learning_starts: Warmup phase to fill the replay buffer\n",
        "        before starting the optimization.\n",
        "    :param exploration_initial_eps: The initial exploration rate\n",
        "    :param exploration_final_eps: The final exploration rate\n",
        "    :param exploration_fraction: The fraction of the number of steps\n",
        "        during which the exploration rate is annealed from\n",
        "        initial_eps to final_eps.\n",
        "        After this many steps, the exploration rate remains constant.\n",
        "    :param n_timesteps: Number of timesteps in total\n",
        "    :param update_interval: How often to update the Q-network\n",
        "        (every update_interval steps)\n",
        "    :param learning_rate: The learning rate to use for the optimizer\n",
        "    :param batch_size: The minibatch size\n",
        "    :param gamma: The discount factor\n",
        "    :param n_hidden_units: Number of units for each hidden layer\n",
        "        of the Q-Network.\n",
        "    :param n_eval_episodes: The number of episodes to evaluate the policy on\n",
        "    :param evaluation_interval: How often to evaluate the policy\n",
        "    :param eval_exploration_rate: The exploration rate to use during evaluation\n",
        "    :param seed: Random seed for the pseudo random generator\n",
        "    :param eval_render_mode: The render mode to use for evaluation\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    # Seed Numpy as PyTorch pseudo random generators\n",
        "    # Seed Numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    th.manual_seed(seed)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym.make(env_id)\n",
        "    # For highway env\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    assert isinstance(env.action_space, spaces.Discrete)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(env_id, render_mode=eval_render_mode)\n",
        "    eval_env = gym.wrappers.FlattenObservation(eval_env)\n",
        "    eval_env.reset(seed=seed)\n",
        "    eval_env.action_space.seed(seed)\n",
        "\n",
        "    # Create the q-network\n",
        "    q_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Create the target network\n",
        "    q_target_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Copy the parameters of the q-network to the target network\n",
        "    q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "    # For flappy bird\n",
        "    if env.observation_space.dtype == np.float64:\n",
        "        q_net.double()\n",
        "        q_target_net.double()\n",
        "\n",
        "    # Create the optimizer, we only optimize the parameters of the q-network\n",
        "    optimizer = th.optim.Adam(q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create the Replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_size, env.observation_space, env.action_space)\n",
        "    # Reset the env\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    for current_step in range(1, n_timesteps + 1):\n",
        "        # Update the current exploration schedule (update the value of epsilon)\n",
        "        exploration_rate = linear_schedule(\n",
        "            exploration_initial_eps,\n",
        "            exploration_final_eps,\n",
        "            current_step,\n",
        "            int(exploration_fraction * n_timesteps),\n",
        "        )\n",
        "        # Do one step in the environment following an epsilon-greedy policy\n",
        "        # and store the transition in the replay buffer\n",
        "        obs = collect_one_step(\n",
        "            env,\n",
        "            q_net,\n",
        "            replay_buffer,\n",
        "            obs,\n",
        "            exploration_rate=exploration_rate,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Update the target network\n",
        "        # by copying the parameters from the Q-network every target_network_update_interval steps\n",
        "        if (current_step % target_network_update_interval) == 0:\n",
        "            q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        # Update the Q-network every update_interval steps\n",
        "        # after learning_starts steps have passed (warmup phase)\n",
        "        if (current_step % update_interval) == 0 and current_step > learning_starts:\n",
        "            # Do one gradient step\n",
        "            dqn_update(q_net, q_target_net, optimizer, replay_buffer, batch_size, gamma=gamma)\n",
        "\n",
        "        if (current_step % evaluation_interval) == 0:\n",
        "            print()\n",
        "            print(f\"Evaluation at step {current_step}:\")\n",
        "            print(f\"exploration_rate={exploration_rate:.2f}\")\n",
        "            # Evaluate the current greedy policy (deterministic policy)\n",
        "            evaluate_policy(eval_env, q_net, n_eval_episodes, eval_exploration_rate=eval_exploration_rate)\n",
        "            # Save a checkpoint\n",
        "            th.save(q_net.state_dict(), f\"../logs/q_net_checkpoint_{env_id}_{current_step}.pth\")\n",
        "    return q_net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14af9427-8166-40fa-a581-51599d3be306",
      "metadata": {
        "id": "14af9427-8166-40fa-a581-51599d3be306"
      },
      "source": [
        "## Train DQN agent with target network on CartPole env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55922877-c7b1-4772-ad1a-e4c1f3ac67d6",
      "metadata": {
        "id": "55922877-c7b1-4772-ad1a-e4c1f3ac67d6"
      },
      "outputs": [],
      "source": [
        "# Tuned hyperparameters from the RL Zoo3 of the Stable Baselines3 library\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=10,\n",
        "    learning_starts=1000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.04,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=80_000,\n",
        "    update_interval=2,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    n_eval_episodes=10,\n",
        "    evaluation_interval=5000,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2022,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52450a1f-60b4-4098-bc64-5860476a50cc",
      "metadata": {
        "id": "52450a1f-60b4-4098-bc64-5860476a50cc"
      },
      "source": [
        "### Visualize the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e18bf9-e3e4-495d-be4b-6acf0ae28ee1",
      "metadata": {
        "id": "56e18bf9-e3e4-495d-be4b-6acf0ae28ee1"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.0\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "# Optional: load checkpoint\n",
        "# q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=64)\n",
        "# q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_CartPole-v1_75000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(video_folder, prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642e02d8-bd9c-4a07-9a0d-86748dcdd391",
      "metadata": {
        "id": "642e02d8-bd9c-4a07-9a0d-86748dcdd391"
      },
      "source": [
        "## Training DQN agent on flappy bird:\n",
        "\n",
        "This is a gymnasium environment by Antonin Raffin.\n",
        "\n",
        "You can go in the [GitHub repo](https://github.com/araffin/flappy-bird-gymnasium/tree/patch-1) to learn more about this environment.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://raw.githubusercontent.com/markub3327/flappy-bird-gymnasium/main/imgs/dqn.gif\" width=\"300\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e9703d-7b5b-4188-9737-786476a627ec",
      "metadata": {
        "id": "75e9703d-7b5b-4188-9737-786476a627ec"
      },
      "outputs": [],
      "source": [
        "!pip install \"flappy-bird-gymnasium @ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a0eef2-d652-4224-86c6-8f669e82ec3e",
      "metadata": {
        "id": "e2a0eef2-d652-4224-86c6-8f669e82ec3e"
      },
      "outputs": [],
      "source": [
        "import flappy_bird_gymnasium  # noqa: F401"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WARNING: this may take some time, make sure you have tested your implementation on CartPole first!\n",
        "\n",
        "If it takes too long, try this at home (with a GPU)"
      ],
      "metadata": {
        "id": "D0MGTMYmEADf"
      },
      "id": "D0MGTMYmEADf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf79677e-561c-4bb5-95d2-d4a99eb0579c",
      "metadata": {
        "id": "bf79677e-561c-4bb5-95d2-d4a99eb0579c"
      },
      "outputs": [],
      "source": [
        "env_id = \"FlappyBird-v0\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=250,\n",
        "    learning_starts=10_000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.03,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=500_000,\n",
        "    update_interval=4,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=128,\n",
        "    gamma=0.98,\n",
        "    n_eval_episodes=5,\n",
        "    evaluation_interval=50000,\n",
        "    n_hidden_units=256,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2023,\n",
        "    eval_render_mode=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a08cc96-2601-49dc-921c-44e2a048df81",
      "metadata": {
        "id": "9a08cc96-2601-49dc-921c-44e2a048df81"
      },
      "source": [
        "### Record a video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e191f2-e3b3-41d0-bff2-50e1b9c9d19c",
      "metadata": {
        "id": "e9e191f2-e3b3-41d0-bff2-50e1b9c9d19c"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.00\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "\n",
        "# Optional: load checkpoint\n",
        "q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=256)\n",
        "# Convert weights from float32 to float64 to match flappy bird obs\n",
        "q_net.double()\n",
        "q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_FlappyBird-v0_200000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(video_folder, prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a60230-6477-4318-8d60-10f6eada6064",
      "metadata": {
        "id": "69a60230-6477-4318-8d60-10f6eada6064"
      },
      "source": [
        "### Going further\n",
        "\n",
        "- analyse the learned q-values\n",
        "- explore different value for the target update, use soft update instead of hard-copy\n",
        "- experiment with Huber loss (smooth l1 loss) instead of l2 loss (mean squared error)\n",
        "- play with different environments\n",
        "- implement a CNN to play flappybird from pixels (need to stack frames)\n",
        "- implement DQN extensions (double Q-learning, prioritized experience replay, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be2cc11-79e6-40c6-ab61-f78fec498315",
      "metadata": {
        "id": "6be2cc11-79e6-40c6-ab61-f78fec498315"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you have seen how to implement the DQN algorithm (update rule and training loop) using all the components from part I (replay buffer, epsilon-greedy exploration strategy, Q-Network, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c3f25b-4eed-456a-b4f7-68ce16f56763",
      "metadata": {
        "id": "89c3f25b-4eed-456a-b4f7-68ce16f56763"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}